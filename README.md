# E-commerce Lakehouse Project

- README em [PortuguÃªs](README.pt-br.md).

## ğŸ“Œ Overview

**Lakehouse** project for e-commerce using a dataset from Kaggle.  
The workflow includes data ingestion, transformation, and availability using **AWS S3**, **PostgreSQL**, **Apache Airflow**, and **dbt**.  
The environment was hosted **locally** on a Linux virtual machine created with **Oracle VirtualBox**, ensuring isolation, portability, and full control over the infrastructure.  
The architecture follows the **Medallion** pattern (Staging, Trusted, and Refined), ensuring organized, clean, and ready-to-use data for advanced analytics and BI visualizations.

## âš™ï¸ Technologies

- **AWS S3** â€“ Raw data storage.
- **Docker** â€“ Hosting PostgreSQL and Apache Airflow services.
- **PostgreSQL** â€“ Relational database.
- **Apache Airflow** â€“ Pipeline orchestration.
- **dbt** â€“ Data transformations, documentation, and testing.

## ğŸš€ Workflow

1. **Ingestion:** Upload Kaggle data to S3 and load into PostgreSQL via Airflow.
2. **Transformation:** Data modeling and cleaning with dbt, creating the Staging, Trusted, and Refined layers.
3. **Availability:** Data ready for analysis in tools like Power BI or Machine Learning models.

## ğŸ“Š Dataset

- Source: [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce/)
- Includes orders, customers, products, payments, and reviews.

## ğŸ›ï¸ Project Architecture

![Project Architecture](assets/excalidraw_image.png)
